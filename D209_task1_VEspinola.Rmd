---
title: "Mitigating Churn"
author: "Victoria Espinola"
date: "2022-10-17"
output: html_document
---

```{r setup, include=FALSE, warning= FALSE}
knitr::opts_chunk$set(
    echo        = TRUE,
    message     = F,
    warning     = F,
    paged.print = FALSE
)
```

###### A1. According to the data dictionary churn rate can be has high as 25% per year and cost 10 times more to aquire new customers than it does to retain an existing customer. Stakeholders often try to mitigate the cost by early intervention inncentive techniques aiming to retain existing customers. The k-nearest neighbor or KNN process uses Euclidean distance between an observation's k nearest neighbors to classify an observation as either a customer that will churn or be retained. 

###### A2. The goal of the analysis is to determine if we can get beat a baseline model accuracy of 73% ,obtained from predicting all customers were retained since 6564 customers were retained out of the 8950 observations remaining after cleaning. If this is the case the model can then be used for early intervention techniques such as financial incentives for customers that are predicted to churn or other intervention techniques chosen by the business stakeholders.

###### B1. K nearest neighbor or KNN is an algorithm that classifies an observation based on information from "nearby" observations. The algorithm calculates the nearness using the euclidean distance between observations. Once the nearest neighbors are found the majority class from the k nearest observations is used as the predicted class for the new observation.The outcome of this method will be used to determine which customers may churn. 

###### B2. One assumption of the KNN classification method is that all variables are numeric so that they can be used to calculate distance. 

###### B3. The following packages were used for this project and their justifications are given in the comments below. 

```{r, echo = TRUE}
library(tidyr)  #This package is for tidying the data and making outputs more readable.
library(tidymodels) #This package assists with train/test split.
library(class) #This package supports the KNN algorithm. 
library(janitor) #This package helps to prepare the data 
library(caret) #This package helps to prepare the data by dummying each categorical data point
library(pROC) #The package helps to create the ROC curve and calculate the AUC value
source("cleaner_copy.R") #This is a cleaning script I wrote to clean the given data set. 
```

###### C1. One data processing goal is to eliminate outliers from the data set. Since KNN relies on distance, all outliers left in the data set heavily influence the predictions. 

###### C2. The following shows the initial data set being brought in and eliminates the uid, case_order, lat, lng, state, county, and time zone variables. The uid and case_order variables were selected for elimination because they are used primarily for identifying customers and not customer attributes or habits. Additionally, lat, lng, state, county, and time zone seemed redundant, so zip was kept in the final data set.

```{r}
churn_data <- read.csv("churn_clean.csv", header = TRUE) %>% 
  janitor::clean_names() %>%
  churn_cleaning() %>%
  select(-uid, -case_order, -lat, -lng, -state, -county,-time_zone)
```

###### The following is the summary statistic of each variable and identifies if the variable is continuous or categorical. 

```{r}
skimr::skim(churn_data)
```

###### C3. To prepare the data, first one-hot encoding was used for factor variables, the dummied variables were put into a new data frame and binded back column wise with numeric variables, and finally the non-dummied categorical variables were removed.  

```{r, echo=TRUE}
#Dummy variables using one-hot encoding. 
one_hot_dummy <- dummyVars(
  " ~.",
  data = (churn_data %>%
            select(where(is.factor) & !contains("churn")) #churn is not included in one-hot encoding since                                                               all non-dummied variables will be removed. 
          )
  )
dummy_df <- data.frame(
  predict(
    one_hot_dummy,
    newdata = (churn_data %>%
                 select(where(is.factor) & !contains("churn")) #creates new df with dummied variables. 
               )
    )
)

final_data_set <- dummy_df %>% bind_cols(churn_data) %>% #put together numeric with dummied variables
  select(!where(is.factor), churn)%>%  #remove all non-dummied variables 
  janitor::clean_names() 


#normalizing numeric data, makes scale between 0 and 1

scaled_ds <- final_data_set %>% 
    mutate(
    across(where(is.numeric), ~(.x-min(.x))/(max(.x)-min(.x))
      )
    )

```

###### C4. The final data set has been normalized and dummy variables have categorical features have been one hot encoded. 
```{r}
skimr::skim(scaled_ds)

#write.csv(scaled_ds, "scaled_ds.csv")
```

###### D1. The following is setting a seed to have reproducible results and will split the data into testing and training set. (DataCamp and Delpiaz)
```{r}
set.seed(123) #Seed is to maintain consistant results and will help keep tie breakers the same as tie breakers are random and not based on algorithm. 
```


```{r}
splits <- initial_split(scaled_ds)
train_data <- training(splits) %>% select(-churn)
churn_train <- training(splits)$churn
test_data <- testing(splits) %>% select(-churn)
churn_test <- testing(splits)$churn
```

```{r}
#code that writes the csv for final submission. 
#write.csv(training(splits), "train_data.csv")
#write.csv(testing(splits), "test_data.csv")
```

###### D2. The model will be assessed by its accuracy but first an appropriate value of k should be found. The following loop will take values between 1 and the square root of the number of values in the training set (DataCamp course)

###### D3. The follow code shows the KNN technique starting with selecting the best value of k neighbors to consider based on accuracy. 
```{r, echo = TRUE}
#Setting K value range
k = 1:sqrt(nrow(train_data))

#Preparing to find accuracy for the loop 
accu_k = rep(x = 0, times= length(k))

#Accuracy function will be calculated from when the actual and predicted values are the same. 
calc_class_acc  = function(actual, prediction) {
  mean(actual == prediction)
}

#Loop for determining the best k value, at which value is the accuracy the largest. 
for (t in seq_along(k)) {
  pred = knn(train = train_data,
             test = test_data,
             cl = churn_train,
             k = k[t])
  accu_k[t] = calc_class_acc(churn_test, pred)
}
```

###### The following shows the accuracy of the model based on various k values. 
```{r}
plot(accu_k, type = "b", col = "red", 
     xlab = "k neighbors", ylab = "Accuracy Rate", 
     main = "(Test) Accuracy Rate for k = 1 to 81")

abline(h = max(accu_k), col = "blue")
```

###### The graph shows that the model reaches its best accuracy between 20 and 40, this value is optimial since it will produce a model that will not be overfit. The following are the calculations used to determine the exact point location. 

```{r}
best_k = min(which(accu_k == max(accu_k))) #finding the first instance with the highest accuracy rate and lowest value of k. 
best_k #prints values of k. 
```

###### The final KNN model is shown below using the k value of 23. 
```{r}
final_knn_prob <- knn(train = train_data,
                 test = test_data,
                 k = 23, 
                 cl = churn_train,
                 prob = TRUE)
```

###### E1. The final classification model that has a k-value of 23 has an accuracy of 84% and AUC value is 82.2%. That means that the model misclassifies 16% of the data that it is given. 
```{r}
conf_matrix <- table(final_knn_prob, churn_test)
conf_matrix

roc(final_knn_prob, attributes(final_knn_prob)$prob, plot = TRUE, legacy.axes = TRUE, percent = TRUE, 
    xlab = "False Positive Percent", ylab = "True Positive Percent", print.auc = TRUE,print.auc.y = 95)

```

###### E2. The model incorrectly classifiies 16% of the data. Specifically, 357 customers were incorrectly classified, with the largest group being 315 customers that were predicted not to churn, but actually did churn. Similarly, the other 42 customers that were misclassified were predicted to churn, but they did not. The KNN algorithm used from the class package did not allow for the distance metric used so a different metric may lead to better results. 

###### E3. One limitation of my data analysis method is that the data is skewed and has more observations of customers who do not churn compared to customers who do churn. A follow up model should include a resampling technique that will help even out the skewed data and may lead to better predictions. 

###### E4. The purpose of the analysis was to help stakeholders decide whether to move forward with implementing a model that would improve churn prediction accuracy. Since our model accuracy is 84%, better than the 73% accuracy of the baseline model that comes from predicting all customers wil stay, the model could prove useful in helping the business in their identification of churning customers and implementation of an intervention program.

###### H. Acknowledgement and Citations
###### 1. Brett Lantz. https://www.datacamp.com/users/sign_in?redirect=http%3A%2F%2Fapp.datacamp.com%2Flearn%2Fcourses%2Fsupervised-learning-in-r-classification
###### 2. Dalpiaz, D. (2020, October 28). Chapter 12 k-Nearest Neighbors | R for Statistical Learning. Retrieved October 14, 2022, from https://daviddalpiaz.github.io/r4sl/knn-class.html