---
title: "Mitigating Churn"
author: "Victoria Espinola"
date: "2022-10-14"
output: html_document
---

```{r setup, include=FALSE, warning= FALSE}
knitr::opts_chunk$set(
    echo        = TRUE,
    message     = F,
    warning     = F,
    paged.print = FALSE
)
```

###### A1. According to the data dictionary churn rate can be has high as 25% per year and cost 10 times more to aquire new customers than it does to retain an existing customer. Stakeholders often try to mitigate the cost by early intervention aiming to retain existing customers. The k- nearest neighbor or KNN process uses a distance to calculate the probability that a customer will discontinue services based on a training model.  

###### A2. The goal of the analysis is to determine if early intervention techniques can be implemented using a KNN analysis. The KNN model will show which groups are most at risk, those who are identified as churning and who are correctly predicted to churn as well as customers who are predicted to retain services but will actually churn. The feasbility of using the KNN model will be determined by the model accuracy. 

###### B1. K nearest neighbor or KNN is an algorithm that classifies an observation based on information from "nearby" observations. The algorithm calculates the nearness using the euclidean distance between observations. Once the nearest neighbors are found the majority class from the k nearest observations is used as the predicted class for the new observation.The outcome of this method will be used to determine which customers may churn. 

###### B2. One assumption of the KNN classification method is that all variables are numeric so that they can be used to calculate distance. 

###### B3. The following packages were used for this project and their justifications are given in the comments below. 

```{r, echo = TRUE}
library(tidyr)  #This package is for tidying the data and making outputs more readable.
library(tidymodels) #This package assists with train/test split.
library(class) #This package supports the KNN algorithm. 
library(janitor) #This package helps to prepare the data 
library(caret) #This package helps to prepare the data by dummying each categorical data point
library(pROC) #The package helps to create the ROC curve and calculate the AUC value
source("cleaner_copy.R") #This is a cleaning script I wrote to clean the given data set. 
```

###### C1. One data processing goal is to eliminate outliers from the data set. Since KNN relies on distance, all outliers left in the data set heavily influence the predictions. 

###### C2. The follow shows the initial data set being brought in and eliminate the uid, case_order, lat, lng, state, county, and time zone variables. The uid and case_order variables were selected for elimination because they create too much noise in the data and they are used primarily for identifying customers and not customer attributes or habits. Additionally, lat, lng, state, county, and time zone seemed redundant, so zip was kept in the final data set.

```{r}
churn_data <- read.csv("churn_clean.csv", header = TRUE) %>% 
  janitor::clean_names() %>%
  churn_cleaning() %>%
  select(-uid, -case_order, -lat, -lng, -state, -county,-time_zone)
```

###### The following is the summary statistic of each variable and identifies if the variable is continuous or categorical. 

```{r}
skimr::skim(churn_data)
```

###### C3. To prepare the data, first one-hot encoding was used for factor variables, the dummied variables were put into a new data frame and binded back column wise with numeric variables, and finally the non-dummied categorical variables were removed.  

```{r, echo=TRUE}
#Dummy variables using one-hot encoding. 
one_hot_dummy <- dummyVars(
  " ~.",
  data = (churn_data %>%
            select(where(is.factor) & !contains("churn")) #churn is not included in one-hot encoding since                                                               all non-dummied variables will be removed. 
          )
  )
dummy_df <- data.frame(
  predict(
    one_hot_dummy,
    newdata = (churn_data %>%
                 select(where(is.factor) & !contains("churn")) #creates new df with dummied variables. 
               )
    )
)

final_data_set <- dummy_df %>% bind_cols(churn_data) %>% #put together numeric with dummied variables
  select(!where(is.factor), churn)%>%  #remove all non-dummied variables 
  janitor::clean_names() 


#normalizing numeric data, makes scale between 0 and 1

scaled_ds <- final_data_set %>% 
    mutate(
    across(where(is.numeric), ~(.x-min(.x))/(max(.x)-min(.x))
      )
    )

```

###### C4. The final data set as dummied variables and has been normalized. 
```{r}
skimr::skim(scaled_ds)

write.csv(scaled_ds, "scaled_ds.csv")
```

###### D1. The following is setting a seed to have reproducible results and has split the data into testing and training set. (DataCamp and Delpiaz)
```{r}
set.seed(123) #Seed is to maintain consistant results and will help keep tie breakers the same as tie breakers are random and not based on algorithm. 
```


```{r}
splits <- initial_split(scaled_ds)
train_data <- training(splits) %>% select(-churn)
churn_train <- training(splits)$churn
test_data <- testing(splits) %>% select(-churn)
churn_test <- testing(splits)$churn
```

###### D2. The model will be assessed by its accuracy, first an appropriate value of k should be found. The following loop will take values between 1 and the square root of the number of values in the training set (Data Camp course)

###### D3. The follow code shows the KNN technique starting with selecting the best value of k neighbors to consider. 
```{r, echo = TRUE}
#Setting K value range
k = 1:sqrt(nrow(train_data))

#Preparing to find accuracy for the loop 
accu_k = rep(x = 0, times= length(k))

#Accuracy function will be calculated from when the actual and predicted values are the same. 
calc_class_acc  = function(actual, prediction) {
  mean(actual == prediction)
}

#Loop for determining the best k value, at which value is the accuracy the largest. 
for (t in seq_along(k)) {
  pred = knn(train = train_data,
             test = test_data,
             cl = churn_train,
             k = k[t])
  accu_k[t] = calc_class_acc(churn_test, pred)
}
```

###### The following shows the accuracy of the model based on various k values. 
```{r}
plot(accu_k, type = "b", col = "red", 
     xlab = "k neighbors", ylab = "Accuracy Rate", 
     main = "(Test) Accuracy Rate for k = 1 to 81")

abline(h = max(accu_k), col = "blue")
```

###### The graph shows that the model reaches its best accuracy between 20 and 40, this value is optimial since it will produce a model that will not be overfit. The following are the calculations used to determine the exact point location. 

```{r}
best_k = min(which(accu_k == max(accu_k))) #finding the first instance with the highest accuracy rate and lowest value of k. 
best_k #prints values of k. 
```

###### The final KNN model is shown below. 
```{r}
final_knn_prob <- knn(train = train_data,
                 test = test_data,
                 k = 23, 
                 cl = churn_train,
                 prob = TRUE)
```

###### E1. The final classification model that has a k-value of 23 has an accuracy and AUC value is 82.2%. That means that the model misclassifies 17.8% of the data that it is given. 
```{r}
conf_matrix <- table(final_knn_prob, churn_test)
conf_matrix

roc(final_knn_prob, attributes(final_knn_prob)$prob, plot = TRUE, legacy.axes = TRUE, percent = TRUE, 
    xlab = "False Positive Percent", ylab = "True Positive Percent", print.auc = TRUE,print.auc.y = 95)

```

###### E2. The model incorrectly classifiies 17.8% of the data. Specifically, 357 customers were incorrectly classified, with the largest group being 315 customers. These customers were predicted not to churn, but actually did churn. Similarly, the other 42 customers that were misclassified were predicted to churn, but they did not. The KNN algorithm used from the class package did not allow for changing threshold values.  

###### E3. One limitation of data analysis method is that since KNN is a computational method the algorithm can result in a tie when there is not a consenous vote for classification; ties are broken at random and that can cause some errors in classification. 

###### E4. The question posed at the outset was to help stakeholder decide to implement early interventions to prevent customer churn; given the results of the data I support going forward with early intervention methods. Upon request further analysis should be considered with a change in threshold values to meet stakeholder goals. 


###### H. Acknowledgement and Citations
###### 1. Brett Lantz. https://www.datacamp.com/users/sign_in?redirect=http%3A%2F%2Fapp.datacamp.com%2Flearn%2Fcourses%2Fsupervised-learning-in-r-classification
###### 2. Dalpiaz, D. (2020, October 28). Chapter 12 k-Nearest Neighbors | R for Statistical Learning. Retrieved October 14, 2022, from https://daviddalpiaz.github.io/r4sl/knn-class.html