% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Improving Churn Model with Random Forest},
  pdfauthor={Victoria Espinola},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Improving Churn Model with Random Forest}
\author{Victoria Espinola}
\date{2022-10-21}

\begin{document}
\maketitle

A1. According to the churn data dictionary, the churn rate for
telecommunications customers can be as high as 25\% per year and it can
cost up to 10 times as much to acquire a new customer than retaining an
existing customer. A KNN model was applied to the customer churn data
set in task one and had an accuracy rate of 84\% for the outcome churn.
While this anaylsis was enough to support moving forward with early
intervention to retain at-risk customers, the goal of this analysis is
to improve upon that classification analysis by appling a random forest
model.

A2. The goal of this analysis is to improve upon the KNN model and
create a random forest model that will have a better accuracy than the
KNN model. If the random forest model has an accuracy higher than 84\%,
then it will be an improvement on the KNN model. The final model will be
given to stakeholders to isolate target customers for intervention and
show the predictor variables that have the most effect on churn outcome.

B1. A random forest method trains a model based on creating individual
decision trees from randomly selected subsets of the predictor variables
from the training data. The collection of trees is then put together as
an ensemble and the majority class is used to generate the predictive
class. The ensemble can be improved upon by tuning the hyperparameters,
and thus can increase the classification accuracy (Applied Predictive
Modeling, M. Kuhn, section 8.5). According to section 4 of Machine
Learning with Tree-Based Models in R, random forest is a good
``out-of-the-box'' performer, hence, the final model is expected to
perform better than the KNN model from task 1 (DataCamp).

B2. One assumption of the random forest method is that low variance
variables need to be removed to avoid overfitting the model (Datacamp,
Machine Learning with Caret, M. Kuhn). One way to recongnize overfitting
is observing that the model has a low in sample error but high out of
sample error.

B3. The libaries used for this analysis include, tidyverse, tidymodels,
janitor, and vip. These libraries were used because they contain the
function for reading in the data, cleaning and preparing the data,
creating a recipe, analyzing the data, and assessing the models
performance. The list of libraries are given in the code below with
comments on how they contributed to the anaylsis.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse) }\CommentTok{\#tidyverse meta package containing useful tools for preparation, dplyr, ggplot2, and readr}
\FunctionTok{library}\NormalTok{(tidymodels) }\CommentTok{\#tidymodels meta package containing useful tools for the analysis, such as tunes, yardstick, recipes, workflow and dials. }
\FunctionTok{library}\NormalTok{(janitor) }\CommentTok{\#janitor cleans names }
\FunctionTok{library}\NormalTok{(vip) }\CommentTok{\#vip is used to find the most important variables. }
\end{Highlighting}
\end{Shaded}

C1. In order to use the random forest method for classification, the
data must have all low variance variables removed, helping the model not
overfit the data. While tree models are robust to outliers, when left in
the data set, they can increase the likilhood of overfitting the data as
the model tend to learn the noise within the data set. Therefore, the
three main goals of the preparation are 1) removing all low variance
variables, 2) normalize the observations using step normalize, and 3)
impute all missing values with the mean.

C2. Since one of the goals of the data preparation steps include
removing all zero variance variables, the entire given data set will be
used to classify churn outcomes. All calculations needed for data
preparation were completed with the recipe function from tidymodels and
no manual calculations were computed. Additionally, the comments show
the purpose for each step of the recipe.

C3. The code below shows the steps for data preparation, the comments
annotate what is being completed in each step. The process for data
preparation can be summed up as, bringing the data set and changing
character variables to factor variables, creating a seed for
reproducibility, splitting the data into training and testing sets, and
finally creating the recipe for feature engineering and data prep as
seen in the comments. The code concludes with a summary of all the
variables and their distrubutions after recipe steps were applied.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{churn\_data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"churn\_clean.csv"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate\_if}\NormalTok{(is.character,as.factor) }\SpecialCharTok{\%\textgreater{}\%}                     \CommentTok{\#bringing in the data, changing nomial variables as factors}
\NormalTok{  janitor}\SpecialCharTok{::}\FunctionTok{clean\_names}\NormalTok{()                                    }\CommentTok{\#standardizing naming conventions}

\CommentTok{\# Create training and testing splits {-}{-}{-}{-}}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{) }
\NormalTok{splits }\OtherTok{\textless{}{-}} \FunctionTok{initial\_split}\NormalTok{(churn\_data, }\AttributeTok{prop =} \FloatTok{0.80}\NormalTok{, }\AttributeTok{strata =}\NormalTok{ churn)  }\CommentTok{\#initial data split, strata ensures each set will have the                                                                      same proportion of churn observations. }

\NormalTok{train\_churn }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(splits)}
\NormalTok{test\_churn  }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(splits)}


\CommentTok{\# Create feature engineering recipe for initial model {-}{-}{-}{-}}
\NormalTok{recipe\_spec }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(churn }\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ train\_churn ) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\#impute the missing variables with the mean}
  \FunctionTok{step\_impute\_mean}\NormalTok{(}\FunctionTok{all\_numeric\_predictors}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}  
  \CommentTok{\#removed any identification variables that do not add insight into the classification model.}
  \FunctionTok{step\_rm}\NormalTok{(county, state, time\_zone, lat, lng, zip) }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\#removed all no variance variable from all predictors }
  \FunctionTok{step\_nzv}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{())}\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\#group together all nominal predictors into an other category when they are retained from low variance, }
  \CommentTok{\#but account for 0.5\% of the data}
  \FunctionTok{step\_other}\NormalTok{(}\FunctionTok{all\_nominal\_predictors}\NormalTok{(),}\AttributeTok{threshold =} \FloatTok{0.005}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\#normalize the numeric data; this step is important to reduce any outliers in the data set. }
  \FunctionTok{step\_normalize}\NormalTok{(}\FunctionTok{all\_numeric\_predictors}\NormalTok{())}

\NormalTok{prepped\_data }\OtherTok{\textless{}{-}}\NormalTok{ recipe\_spec }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{prep}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{bake}\NormalTok{(}\AttributeTok{new\_data =}\NormalTok{ churn\_data) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ janitor}\SpecialCharTok{::}\FunctionTok{clean\_names}\NormalTok{()}
\NormalTok{skimr}\SpecialCharTok{::}\FunctionTok{skim}\NormalTok{(prepped\_data)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}ll@{}}
\caption{Data summary}\tabularnewline
\toprule()
\endhead
Name & prepped\_data \\
Number of rows & 10000 \\
Number of columns & 44 \\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ & \\
Column type frequency: & \\
factor & 24 \\
numeric & 20 \\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ & \\
Group variables & None \\
\bottomrule()
\end{longtable}

\textbf{Variable type: factor}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1765}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0980}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1373}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0784}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0882}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.4216}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
skim\_variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
n\_missing
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
complete\_rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ordered
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
n\_unique
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
top\_counts
\end{minipage} \\
\midrule()
\endhead
customer\_id & 0 & 1 & FALSE & 2 & oth: 9999, A04: 1 \\
interaction & 0 & 1 & FALSE & 2 & oth: 9999, 000: 1 \\
uid & 0 & 1 & FALSE & 2 & oth: 9999, 000: 1 \\
city & 0 & 1 & FALSE & 2 & oth: 9966, Hou: 34 \\
area & 0 & 1 & FALSE & 3 & Sub: 3346, Rur: 3327, Urb: 3327 \\
job & 0 & 1 & FALSE & 2 & oth: 9974, Sal: 26 \\
marital & 0 & 1 & FALSE & 5 & Div: 2092, Wid: 2027, Sep: 2014, Nev:
1956 \\
gender & 0 & 1 & FALSE & 3 & Fem: 5025, Mal: 4744, Non: 231 \\
techie & 0 & 1 & FALSE & 2 & No: 8321, Yes: 1679 \\
contract & 0 & 1 & FALSE & 3 & Mon: 5456, Two: 2442, One: 2102 \\
port\_modem & 0 & 1 & FALSE & 2 & No: 5166, Yes: 4834 \\
tablet & 0 & 1 & FALSE & 2 & No: 7009, Yes: 2991 \\
internet\_service & 0 & 1 & FALSE & 3 & Fib: 4408, DSL: 3463, Non:
2129 \\
phone & 0 & 1 & FALSE & 2 & Yes: 9067, No: 933 \\
multiple & 0 & 1 & FALSE & 2 & No: 5392, Yes: 4608 \\
online\_security & 0 & 1 & FALSE & 2 & No: 6424, Yes: 3576 \\
online\_backup & 0 & 1 & FALSE & 2 & No: 5494, Yes: 4506 \\
device\_protection & 0 & 1 & FALSE & 2 & No: 5614, Yes: 4386 \\
tech\_support & 0 & 1 & FALSE & 2 & No: 6250, Yes: 3750 \\
streaming\_tv & 0 & 1 & FALSE & 2 & No: 5071, Yes: 4929 \\
streaming\_movies & 0 & 1 & FALSE & 2 & No: 5110, Yes: 4890 \\
paperless\_billing & 0 & 1 & FALSE & 2 & Yes: 5882, No: 4118 \\
payment\_method & 0 & 1 & FALSE & 4 & Ele: 3398, Mai: 2290, Ban: 2229,
Cre: 2083 \\
churn & 0 & 1 & FALSE & 2 & No: 7350, Yes: 2650 \\
\bottomrule()
\end{longtable}

\textbf{Variable type: numeric}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.2333}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.1111}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.1556}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0667}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0556}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0667}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0667}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0667}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0556}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0667}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
skim\_variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
n\_missing
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
complete\_rate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
mean
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
sd
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p0
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p25
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p50
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p75
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p100
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
hist
\end{minipage} \\
\midrule()
\endhead
case\_order & 0 & 1 & 0.00 & 1.00 & -1.73 & -0.86 & 0.00 & 0.87 & 1.74 &
▇▇▇▇▇ \\
population & 0 & 1 & 0.00 & 1.00 & -0.68 & -0.63 & -0.48 & 0.23 & 7.05 &
▇▁▁▁▁ \\
children & 0 & 1 & 0.00 & 1.01 & -0.98 & -0.98 & -0.51 & 0.43 & 3.71 &
▇▃▁▁▁ \\
age & 0 & 1 & 0.01 & 1.00 & -1.69 & -0.87 & 0.00 & 0.87 & 1.75 &
▇▇▇▇▇ \\
income & 0 & 1 & 0.00 & 1.00 & -1.40 & -0.73 & -0.23 & 0.48 & 7.76 &
▇▂▁▁▁ \\
outage\_sec\_perweek & 0 & 1 & 0.00 & 1.00 & -3.31 & -0.67 & 0.00 & 0.66
& 3.75 & ▁▅▇▂▁ \\
email & 0 & 1 & 0.00 & 1.00 & -3.65 & -0.67 & -0.01 & 0.65 & 3.63 &
▁▂▇▂▁ \\
contacts & 0 & 1 & 0.00 & 1.00 & -1.01 & -1.01 & 0.01 & 1.02 & 6.10 &
▇▂▁▁▁ \\
yearly\_equip\_failure & 0 & 1 & 0.00 & 1.00 & -0.63 & -0.63 & -0.63 &
0.94 & 8.79 & ▇▁▁▁▁ \\
tenure & 0 & 1 & 0.00 & 1.00 & -1.27 & -1.00 & 0.04 & 1.02 & 1.42 &
▇▂▁▃▆ \\
monthly\_charge & 0 & 1 & 0.00 & 1.00 & -2.16 & -0.76 & -0.12 & 0.66 &
2.74 & ▂▇▆▃▁ \\
bandwidth\_gb\_year & 0 & 1 & 0.00 & 1.00 & -1.48 & -0.98 & -0.05 & 1.01
& 1.73 & ▇▃▁▆▅ \\
item1 & 0 & 1 & -0.01 & 1.00 & -2.41 & -0.48 & -0.48 & 0.48 & 3.37 &
▃▇▇▃▁ \\
item2 & 0 & 1 & -0.01 & 1.00 & -2.43 & -0.49 & 0.47 & 0.47 & 3.37 &
▃▇▇▃▁ \\
item3 & 0 & 1 & -0.01 & 1.00 & -2.42 & -0.48 & -0.48 & 0.49 & 4.36 &
▃▆▇▁▁ \\
item4 & 0 & 1 & 0.00 & 1.00 & -2.42 & -0.48 & -0.48 & 0.49 & 3.41 &
▃▇▇▃▁ \\
item5 & 0 & 1 & 0.00 & 1.00 & -2.43 & -0.48 & -0.48 & 0.49 & 3.42 &
▃▇▇▃▁ \\
item6 & 0 & 1 & 0.00 & 1.00 & -2.42 & -0.48 & -0.48 & 0.48 & 4.35 &
▃▆▇▁▁ \\
item7 & 0 & 1 & 0.01 & 1.00 & -2.43 & -0.49 & 0.48 & 0.48 & 3.40 &
▃▇▇▃▁ \\
item8 & 0 & 1 & 0.00 & 0.99 & -2.41 & -0.48 & -0.48 & 0.49 & 4.36 &
▃▆▇▁▁ \\
\bottomrule()
\end{longtable}

C4. The code below shows the script for creating the csv file with all
the data to be uploaded for the final submission.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(prepped\_data, }\StringTok{"prepped\_data.csv"}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

D1. In the previous chunk of code, the character variables have been
converted to factors and the data had an initial 80/20 split with a
stratification on the churn outcome. From the initial split the training
and testing sets were created. The code below applies the preparation
reciepe steps to the split data and outputs the new preppred\_train and
prepped\_test data sets that will be used for analysis.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Prep data, clean the names}
\NormalTok{prepped\_train\_data }\OtherTok{\textless{}{-}}\NormalTok{ recipe\_spec }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{prep}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{bake}\NormalTok{(}\AttributeTok{new\_data =}\NormalTok{ train\_churn) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ janitor}\SpecialCharTok{::}\FunctionTok{clean\_names}\NormalTok{()}
\NormalTok{prepped\_test\_data }\OtherTok{\textless{}{-}}\NormalTok{ recipe\_spec }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{prep}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{bake}\NormalTok{(}\AttributeTok{new\_data =}\NormalTok{ test\_churn) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ janitor}\SpecialCharTok{::}\FunctionTok{clean\_names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Now the prepped data will be writen as CSV files for attachment to the
final submission.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Save prepped data as CSV}
\FunctionTok{write.csv}\NormalTok{(prepped\_train\_data, }\StringTok{"prepped\_train\_data.csv"}\NormalTok{ )}
\FunctionTok{write.csv}\NormalTok{(prepped\_test\_data, }\StringTok{"prepped\_test\_data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

D2. The following analysis technique was implemented from the DataCamp
course, ``Machine Learning with Tree Models, section 4''; start with the
spec model with tuning parameters, set up the number of folds, and
create a tuning grid, then use the grid results to find the best
parameters, in this instance accuracy will be used to assess the best
tuned model parameters. Finally, the tuning specs are passed to the
model spec to finalize the parameters the model will use when trained.
The model accuracy with the finalized hyperparameters will be assessed
using 8-fold cross validation on the training set and then if
satisfactory the model will be fit on the entire training set.

*The following code chunk will not be run during demonstration due to
processing time.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Set tuning parameters for random forest classification.}
\NormalTok{rand\_forest\_spec }\OtherTok{\textless{}{-}} \FunctionTok{rand\_forest}\NormalTok{(}
  \AttributeTok{mtry  =} \FunctionTok{tune}\NormalTok{(), }
  \AttributeTok{trees =} \FunctionTok{tune}\NormalTok{(), }
  \AttributeTok{min\_n =} \FunctionTok{tune}\NormalTok{()}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"classification"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"ranger"}\NormalTok{, }\AttributeTok{importance =} \StringTok{"permutation"}\NormalTok{) }\CommentTok{\#needed to create variable importance plot}

\CommentTok{\#set seed for reproducibility for folds and tuning grid because both having a random element.}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}

\CommentTok{\#set up tuning grid}
\NormalTok{test\_grid }\OtherTok{\textless{}{-}}\NormalTok{ tidyr}\SpecialCharTok{::}\FunctionTok{crossing}\NormalTok{(}
  \AttributeTok{mtry  =} \DecValTok{5}\SpecialCharTok{:}\DecValTok{8}\NormalTok{, }\CommentTok{\#best practice is sqrt of number of variables, sqrt(50)\textasciitilde{} 7}
  \AttributeTok{trees =} \FunctionTok{seq}\NormalTok{(}\DecValTok{300}\NormalTok{,}\DecValTok{600}\NormalTok{,}\DecValTok{100}\NormalTok{), }\CommentTok{\#best practice is 500 trees from ranger parsnip doc.}
  \AttributeTok{min\_n =} \DecValTok{9}\SpecialCharTok{:}\DecValTok{15} \CommentTok{\# best practice is value of 10 from ranger parsnip doc.}
\NormalTok{)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{doParallel}\SpecialCharTok{::}\FunctionTok{registerDoParallel}\NormalTok{() }\CommentTok{\#improve the time for the model to run. }
\NormalTok{tune\_results }\OtherTok{\textless{}{-}} \FunctionTok{tune\_grid}\NormalTok{(rand\_forest\_spec, }
\NormalTok{                          recipe\_spec,}
                          \AttributeTok{resamples =}\NormalTok{ tree\_folds,}
                          \AttributeTok{grid      =}\NormalTok{ test\_grid,}
                          \AttributeTok{metrics   =} \FunctionTok{metric\_set}\NormalTok{(accuracy) }\CommentTok{\#parameters optimized by accuracy.}
\NormalTok{                          )}

\CommentTok{\#Select best tune results from the tune grid to be put into the final model. }
\NormalTok{best\_tunes }\OtherTok{\textless{}{-}} \FunctionTok{select\_best}\NormalTok{(tune\_results)}
\CommentTok{\#Including tuned specs from best tunes for the final model. }
\NormalTok{best\_spec }\OtherTok{\textless{}{-}} \FunctionTok{finalize\_model}\NormalTok{(rand\_forest\_spec, best\_tunes)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#set seed for tuned results}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\CommentTok{\#set folds value for resampling}
\NormalTok{tree\_folds }\OtherTok{\textless{}{-}} \FunctionTok{vfold\_cv}\NormalTok{(}\AttributeTok{data =}\NormalTok{ train\_churn, }\AttributeTok{v =} \DecValTok{8}\NormalTok{, }\AttributeTok{strata =}\NormalTok{ churn)}

\CommentTok{\#visualizing the tune results, to show what will be selected.}

\NormalTok{hyper\_param\_list }\OtherTok{\textless{}{-}} \FunctionTok{read\_rds}\NormalTok{(}\StringTok{"model\_tuning.rds"}\NormalTok{)}
\NormalTok{tune\_results }\OtherTok{\textless{}{-}}\NormalTok{ hyper\_param\_list}\SpecialCharTok{$}\NormalTok{tune\_results}
\NormalTok{best\_spec }\OtherTok{\textless{}{-}}\NormalTok{ hyper\_param\_list}\SpecialCharTok{$}\NormalTok{best\_spec}

\FunctionTok{autoplot}\NormalTok{(tune\_results)}
\end{Highlighting}
\end{Shaded}

\includegraphics{D209_task2_VEspinola_files/figure-latex/unnamed-chunk-7-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#finalizing the model}
\NormalTok{final\_wflw }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(recipe\_spec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(best\_spec)  }

\CommentTok{\#add fit resample}
\NormalTok{fit\_resample }\OtherTok{\textless{}{-}} \FunctionTok{fit\_resamples}\NormalTok{(}
\NormalTok{                            final\_wflw,}
                            \AttributeTok{resamples =}\NormalTok{ tree\_folds,}
                            \AttributeTok{metrics   =} \FunctionTok{metric\_set}\NormalTok{(roc\_auc, accuracy)}
\NormalTok{                    )}

\CommentTok{\#using collect metrics to find the models performance}
\FunctionTok{collect\_metrics}\NormalTok{(fit\_resample, }\AttributeTok{summarize =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 6
##   .metric  .estimator  mean     n std_err .config             
##   <chr>    <chr>      <dbl> <int>   <dbl> <chr>               
## 1 accuracy binary     0.893     8 0.00330 Preprocessor1_Model1
## 2 roc_auc  binary     0.951     8 0.00228 Preprocessor1_Model1
\end{verbatim}

D3. The following code is used to assess how the tuned model will
perform on out of sample data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#full model created from work flow with all best tuning parameters}
\NormalTok{full\_model }\OtherTok{\textless{}{-}}\NormalTok{ final\_wflw }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{last\_fit}\NormalTok{(splits)}

\CommentTok{\#visualizing the final full model. }
\NormalTok{full\_model }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{collect\_predictions}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{roc\_curve}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ churn, }\AttributeTok{estimate =}\NormalTok{ .pred\_No) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{autoplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{D209_task2_VEspinola_files/figure-latex/unnamed-chunk-8-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#viewing the metrics on the final full model. }
\NormalTok{full\_model }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{collect\_metrics}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 4
##   .metric  .estimator .estimate .config             
##   <chr>    <chr>          <dbl> <chr>               
## 1 accuracy binary         0.883 Preprocessor1_Model1
## 2 roc_auc  binary         0.949 Preprocessor1_Model1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Returns the workflow and display the most important variables from VIP package. }
\FunctionTok{extract\_workflow}\NormalTok{(full\_model)}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{extract\_fit\_parsnip}\NormalTok{()}\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{vip}\NormalTok{(}\AttributeTok{geom=}\StringTok{"point"}\NormalTok{,}\AttributeTok{num\_features=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{D209_task2_VEspinola_files/figure-latex/unnamed-chunk-8-2.pdf}

E1. The prediction model has an accuracy value of 89.3\% and a ROC AUC
value of 95\%. The accuracy of the model reveals that 10.7\% of the
observations are misclassified.

E2. From the full model using out of sample data, the model has an
accuracy value of 88.3\% and a ROC AUC value of 94.9\%. This means that
when given data that the model has not been learned from, the model will
misclassify 11.4\% of the observations.

E3. One limitation of the analysis technique is that the number of
possible tuning parameter combinations that were used were limited by my
machines capabilities. Since every combination of parameters needs to be
tested by fitting the model to the values, computational time can
quickly become very steep. The hyperparameter values were selected by
applying best practice metrics from ``Tidy Modeling with R'' (Kuhn \&
Silge, 2022) and the R documentation for ranger random forest package.

E4.Overall the random forest model gave an improvement upon the KNN
model from task 1. Stakeholders are still encouraged to seek out
intervention opportunities that will help to mitigate churn rates. It is
also advisable to monitor churn rates based on tenure, monthly charge,
contract, bandwidth, case order, streaming movies, streaming tv,
internet service and multiple service as these variables were selected
by the random forest model as being the most important.

G. Citation

1. Machine Learning with Caret in R:
\url{https://app.datacamp.com/learn/courses/machine-learning-with-caret-in-r}

2. Machine Learning with Tree Models in R:
\url{https://app.datacamp.com/learn/courses/machine-learning-with-tree-based-models-in-r}

3. rand\_forest: Random forest in parsnip: A Common API to Modeling and
Analysis Functions Max Kuhn
\url{https://rdrr.io/cran/parsnip/man/rand_forest.html}

4. Tidy Modeling with R. Silge. \url{https://www.tmwr.org/}

5. Applied Predictive Modeling. Kuhn.
\url{http://appliedpredictivemodeling.com/}

\end{document}
